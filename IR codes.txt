1. Preprocessing, Stopword removal , Normalization 
import os
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
porter = PorterStemmer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())

    processed_tokens = [porter.stem(token) for token in tokens if token.isalnum() and token not in stop_words]
    
    return processed_tokens

def process_files_in_directory(directory_path):
    processed_data = {}
    
    for filename in sorted(os.listdir(directory_path)):
        file_path = os.path.join(directory_path, filename)
        
        if os.path.isfile(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                processed_data[filename] = preprocess_text(text)
    
    return processed_data

directory_path = "/content/drive/MyDrive/YourFolderName"  
processed_files = process_files_in_directory(directory_path)
for filename, tokens in processed_files.items():
    print(f"{filename}: {tokens[:10]}...") 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
2. Sort-based indexing
import os
import re
from collections import defaultdict

class SortBasedIndexing:
    def __init__(self):
        self.term_document_map = defaultdict(list)
        self.documents = []

    def tokenize(self, text):
        return re.findall(r'\b\w+\b', text.lower())

    def add_document(self, doc_id, text):
        tokens = self.tokenize(text)
        self.documents.append((doc_id, tokens))
        for position, term in enumerate(tokens):
            self.term_document_map[term].append((doc_id, position))

    def sort_terms(self):
        sorted_terms = sorted(self.term_document_map.items())
        return sorted_terms

    def create_inverted_index(self):
        inverted_index = defaultdict(list)
        sorted_terms = self.sort_terms()
        for term, postings in sorted_terms:
            inverted_index[term] = sorted(postings)
        return inverted_index

    def display_inverted_index(self, inverted_index):
        for term, postings in inverted_index.items():
            doc_ids = [doc_id for doc_id, _ in postings]
            print(f"{term}: {doc_ids}")

documents_path = "/content/drive/MyDrive/Colab Notebooks/Final Data"

indexing = SortBasedIndexing()

for doc_id, file_name in enumerate(os.listdir(documents_path)):
    file_path = os.path.join(documents_path, file_name)
    if os.path.isfile(file_path): 
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            indexing.add_document(doc_id, content)

inverted_index = indexing.create_inverted_index()
indexing.display_inverted_index(inverted_index)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
3.SPIMI 
import nltk
from google.colab import drive
import os
from collections import defaultdict
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

nltk.download('punkt_tab')

class InMemoryIndexer:
    def __init__(self, input_dir, output_file):
        self.input_dir = input_dir
        self.output_file = output_file
        self.index = defaultdict(list)
        self.doc_map = {}
        self.porter = PorterStemmer()

    def build_index(self):
        for doc_id, filename in enumerate(sorted(os.listdir(self.input_dir))):
            path = os.path.join(self.input_dir, filename)
            self.doc_map[doc_id] = filename
            with open(path, 'r', encoding='utf-8') as f:
                tokens = {self.porter.stem(token) for token in word_tokenize(f.read().lower())}
                for token in tokens:
                    self.index[token].append(doc_id)
        self.write_index()

    def write_index(self):
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for term, doc_ids in sorted(self.index.items()):
                f.write(f"{term} ({len(doc_ids)}) - [{','.join(map(str, sorted(doc_ids)))}]\n")

    def print_doc_map(self):
        for doc_id, filename in self.doc_map.items():
            print(f"Document ID: {doc_id}, Filename: {filename}")

input_dir = "/content/drive/MyDrive/Colab Notebooks/Final Data"
output_file = "/content/drive/MyDrive/Colab Notebooks/single_pass_output.txt"

indexer = InMemoryIndexer(input_dir, output_file)
indexer.build_index()
indexer.print_doc_map()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
4.Logarithmic Indexing 
import os
from collections import defaultdict
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import nltk
nltk.download('punkt')

class LogarithmicDynamicIndexer:
    def __init__(self, output_file, n_indexes=2, n=10):
        self.n_indexes = n_indexes  
        self.n = n  
        self.indexes = self.create_indexes(n_indexes, n)  
        self.filled = []  
        self.z = []
        self.porter = PorterStemmer()
        self.output_file = output_file

    def create_indexes(self, n_indexes, n):
        
        return [[] for _ in range(n_indexes)]

    def tokenize_and_process(self, text):
       
        tokens = word_tokenize(text.lower())
        processed_tokens = [self.porter.stem(token) for token in tokens]
        return [token for token in processed_tokens if token.isalpha()]  

    def merge_add_token(self, token):
        
        self.z.append(token)
        if len(self.z) == self.n:
            self._merge_levels(0)  
            self.z = []  #

    def _merge_levels(self, level):
    
        if level >= self.n_indexes:
            return

        if self.indexes[level] in self.filled:
            self.z += self.indexes[level]
            self.filled.remove(self.indexes[level])

            self.indexes[level] = []
            self._merge_levels(level + 1)
        else:
            self.indexes[level] = self.z
            self.filled.append(self.indexes[level])
    def flush_buffer(self):
        if self.z:
            print(f"Flushing buffer: {self.z}")
            self._merge_levels(0)
            self.z = []

    def logarithmic_merge(self, text):
        tokens = self.tokenize_and_process(text)
        print(f"Tokens after processing: {tokens}")
        for token in tokens:
            self.merge_add_token(token)

    def log_indexes_to_file(self, doc_id, filename):
        with open(self.output_file, 'a', encoding='utf-8') as f:
            f.write(f"\nProcessing Document {doc_id + 1}: {filename}...\n")
            for i, index in enumerate(self.indexes):
                f.write(f"Index level {i}: {index}\n")

def simulate_logarithmic_dynamic_indexing(input_dir, output_file):
    dynamic_indexer = LogarithmicDynamicIndexer(output_file=output_file, n_indexes=2, n=10)

    doc_id = 0
    for dirpath, dirnames, filenames in os.walk(input_dir):
        filenames.sort()
        for filename in filenames:
            file_path = os.path.join(dirpath, filename)

            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()

            print(f"\nProcessing Document {doc_id + 1}: {filename}...")
            dynamic_indexer.logarithmic_merge(text)

            dynamic_indexer.flush_buffer()

            dynamic_indexer.log_indexes_to_file(doc_id, filename)

            doc_id += 1

if __name__ == "__main__":
    input_dir = "/content/drive/MyDrive/Colab Notebooks/Final Data"

    output_file = "/content/drive/MyDrive/Colab Notebooks/output.txt"

    simulate_logarithmic_dynamic_indexing(input_dir, output_file)

    file_path = "/content/drive/MyDrive/Colab Notebooks/output.txt"

    with open(file_path, 'r', encoding='utf-8') as file:
     for line in file:
        print(line.strip())
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
5.Binary Independence Model
import os
from collections import defaultdict

def preprocess(text):
    return text.lower().split()

def build_index(paths):
    index = defaultdict(set)
    for doc_id, path in enumerate(paths):
        with open(path, 'r') as f:
            index[doc_id] = set(preprocess(f.read()))
    return index

def compute_probabilities(index, num_docs):
    probabilities = {}
    terms = {term for doc in index.values() for term in doc}
    for term in terms:
        count = sum(1 for doc in index.values() if term in doc)
        probabilities[term] = (count / num_docs, 1 - count / num_docs)
    return probabilities

def score_doc(query, doc_id, probabilities, index):
    score = 1
    for term in query:
        p_rel, p_not_rel = probabilities.get(term, (0.5, 0.5))
        score *= p_rel / p_not_rel if term in index[doc_id] else (1 - p_rel) / (1 - p_not_rel)
    return score

input_dir = "/path/to/documents"
query = "example query"

docs = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.txt')]
index = build_index(docs)
probs = compute_probabilities(index, len(docs))

query_terms = preprocess(query)
scores = {doc_id: score_doc(query_terms, doc_id, probs, index) for doc_id in index}
sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)

for doc_id, score in sorted_docs:
    print(f"Document {doc_id}: Score = {score:.4f}")
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
6.Vector Space model 
import os, math, nltk
from collections import defaultdict
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')

def preprocess(text):
    ps, stop_words = PorterStemmer(), set(stopwords.words('english'))
    return [ps.stem(word.lower()) for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]

def build_index(paths):
    index, raw_tf = defaultdict(list), defaultdict(lambda: defaultdict(int))
    for doc_id, path in enumerate(paths):
        tokens = preprocess(open(path, 'r').read())
        for term in tokens:
            if doc_id not in index[term]:
                index[term].append(doc_id)
            raw_tf[term][doc_id] += 1
    return index, raw_tf

def compute_tf_idf(index, raw_tf, num_docs):
    idf = {term: math.log(num_docs / len(postings), 2) for term, postings in index.items()}
    tf_idf = {term: {doc_id: (1 + math.log(count, 2)) * idf[term] for doc_id, count in doc_counts.items()}
              for term, doc_counts in raw_tf.items()}
    doc_norms = {doc_id: math.sqrt(sum(weight ** 2 for weight in doc_weights.values()))
                 for doc_id, doc_weights in defaultdict(lambda: defaultdict(float), tf_idf).items()}
    return tf_idf, idf, doc_norms

def process_query(query, tf_idf, idf, doc_norms):
    query_tf = {term: 1 + math.log(query.count(term), 2) for term in set(query)}
    query_vector = {term: tf * idf.get(term, 0) for term, tf in query_tf.items()}
    query_norm = math.sqrt(sum(weight ** 2 for weight in query_vector.values()))
    similarities = defaultdict(float)
    for term, query_value in query_vector.items():
        for doc_id, doc_value in tf_idf.get(term, {}).items():
            similarities[doc_id] += query_value * doc_value
    return sorted({doc: score / (query_norm * doc_norms[doc]) for doc, score in similarities.items()}.items(),
                  key=lambda x: x[1], reverse=True)

input_dir = "/content/drive/MyDrive/fulldata"
doc_paths = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.txt')]
index, raw_tf = build_index(doc_paths)
tf_idf, idf, doc_norms = compute_tf_idf(index, raw_tf, len(doc_paths))
query = preprocess("example query")
results = process_query(query, tf_idf, idf, doc_norms)

for doc_id, score in results:
    print(f"Document {doc_id}: {score:.4f}")
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
7.Support Vector Machine 
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

def load_data(folder):
    return [(open(os.path.join(root, file), 'r', encoding='utf-8').read(), label)
            for label in os.listdir(folder) if os.path.isdir(os.path.join(folder, label))
            for root, _, files in [os.walk(os.path.join(folder, label))]
            for file in files]

data_folder = "path_to_your_folder"
data = load_data(data_folder)
documents, labels = zip(*data)

X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.3, random_state=42)
vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)
X_train_vectors, X_test_vectors = vectorizer.fit_transform(X_train), vectorizer.transform(X_test)

model = LinearSVC().fit(X_train_vectors, y_train)
y_pred = model.predict(X_test_vectors)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Report:\n", classification_report(y_test, y_pred))
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



